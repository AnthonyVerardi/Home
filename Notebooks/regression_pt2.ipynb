{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning 2\n",
    "Jevon Heath, Feb 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ~~Linear~~ Logistic Regression: fitting to a ~~line~~ probability curve\n",
    "Instead of a continuous outcome, we want a categorical response:\n",
    "* Is this email junk or not?\n",
    "* Is this a correct usage or not?\n",
    "* Is the speaker a native speaker or not?\n",
    "* Is the backchannel in question laughter, non-verbal, phrasal, or substantive?\n",
    "\n",
    "For these **classification** questions, the outcome should be a specific category.\n",
    "\n",
    "But a model can also give us the _likelihood_ of that predicted outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assumptions:\n",
    "* continuous values **for independent variables**\n",
    "* ~~a linear relationship~~ **linear independent variables**\n",
    "* ~~multivariate normality~~\n",
    "* no multicollinearity among independent variables\n",
    "* ~~homoskedasticity~~\n",
    "* **independence of observations**\n",
    "* **a large sample size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turns on/off pretty printing \n",
    "%pprint\n",
    "\n",
    "# Every returned Out[] is displayed, not just the last one. \n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn               \n",
    "import nltk \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns        \n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: predicting discrete labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple case: two labels\n",
    "Quick example: Given a reaction time, is the participant young or old?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english = pd.read_csv('../../Class-Exercise-Repo/activity3/english_updated.csv', index_col='Index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english['AgeSubject'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1 = smf.glm(\"AgeSubject ~ RTlexdec + WrittenFrequency\", data=english, family=sm.families.Binomial())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1f = logit1.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit1f.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Complicated case: many labels\n",
    "\n",
    "Now a textbook example using sklearn's pre-loaded data set 20 news group data. \n",
    "- For detailed explanation, see the textbook section:\n",
    " https://jakevdp.github.io/PythonDataScienceHandbook/05.05-naive-bayes.html\n",
    "- The original data set can be downloaded from: http://qwone.com/~jason/20Newsgroups/\n",
    "- sklearn's tutorial on the dataset: https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html\n",
    "\n",
    "Topic classification is our goal: \n",
    "- Given a short text, can we identify topic labels? \n",
    "\n",
    "Text-based classification requires converting **INDIVIDUAL WORDS into a their own features**, which blows up feature space. Some common strategies:\n",
    "\n",
    "- Removing stop words and punctuation (depending on your data and goal) \n",
    "- Limiting word types to top 2000K, 5000K, etc. \n",
    "- Using \"sparse vector\" format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "data = fetch_20newsgroups()   # downloads training data by default: subset='train'. test', 'all'\n",
    "data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(data)\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filenames[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.target[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We'll download subsections of the data, with four categories only, training and test sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc', 'soc.religion.christian',\n",
    "              'sci.space', 'comp.graphics']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories)\n",
    "test = fetch_20newsgroups(subset='test', categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train)\n",
    "dir(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.data[3]\n",
    "# Quick! Which topic is this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target_names\n",
    "train.target_names[train.target[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.target[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.data)\n",
    "len(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is not in DataFrame format, but you could shape it into one if you wanted to: \n",
    "train_df = pd.DataFrame()\n",
    "train_df['target'] = train.target\n",
    "train_df['text'] = train.data\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: how do you extract & represent word-based features from the text?\n",
    "- **Bag-of-words** approach: reduce a document to the words it contains\n",
    "- **Occurrence** features: whether or not each word occurs in a document (0 or 1)\n",
    "- **Count features**:  how many times each word occurs in a document (0 --) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = train_df[:10].copy()   # first 10 rows\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase and then tokenize\n",
    "toy_df['tokens'] = toy_df.text.map(lambda x: nltk.word_tokenize(x.lower()))\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df['god#'] = toy_df.tokens.map(lambda x: x.count('god'))\n",
    "toy_df['believe#'] = toy_df.tokens.map(lambda x: x.count('believe'))\n",
    "toy_df['space#'] = toy_df.tokens.map(lambda x: x.count('space'))\n",
    "toy_df['computer#'] = toy_df.tokens.map(lambda x: x.count('computer'))\n",
    "toy_df['graphics#'] = toy_df.tokens.map(lambda x: x.count('graphics'))\n",
    "toy_df['the#'] = toy_df.tokens.map(lambda x: x.count('the'))\n",
    "toy_df['you#'] = toy_df.tokens.map(lambda x: x.count('you'))\n",
    "toy_df['way#'] = toy_df.tokens.map(lambda x: x.count('way'))\n",
    "toy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now do this for ALL word types in the training data...\n",
    "- Or, more realistically, we could do this for the _n_ most frequent word types (We'll use 3,000)\n",
    "- Then, the word-count columns (3,000 of them!) will be `X_train`. Feed that into the Naive Bayes training algorithm...\n",
    "- But is there a better way?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations\n",
    "1. We need to normalize the values: raw counts are sensitive to text length. \n",
    "2. Some words are going to be frequent across all topics, just because they are common words ('the', 'way', 'talked')\n",
    "   - We could filter our function words, but that goes only so far \n",
    "   - 'space' will be common in one topic, not so in others. 'god' will be common in two, but not in others. How to better capture this? \n",
    "3. The vector is going to be SPARSE: most values will be 0. A DataFrame is not an efficient data structure for this.\n",
    "4. We don't want to do all this manually, word by word! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood with CounterVectorizer and TF-IDF\n",
    "\n",
    "#### Count-vectorize, and then TF-IDF\n",
    "- 3. & 4. as well as tokenization are handled by `CountVectorizer`\n",
    "- 1. & 2. are addressed by `TfidfTransformer`\n",
    "\n",
    "A detour: we will take a look at a detailed illustration of CountVectors and TF-IDF:\n",
    "http://www.pitt.edu/~naraehan/presentation/Movie%20Reviews%20sentiment%20analysis%20with%20Scikit-Learn.html#A-detour:-try-out-CountVectorizer-&-TF-IDF\n",
    "\n",
    "\n",
    "**TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
    "- Textbook section on TF-IDF: https://jakevdp.github.io/PythonDataScienceHandbook/05.04-feature-engineering.html#Text-Features\n",
    "- Better explanation here: http://www.tfidf.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the textbook and our 4 newsgroups. \n",
    "-  **Reminder:  Refer to textbook for explanation!! Link up above.** \n",
    "- `TfidfVectorizer()` used below is a combination of `CountVectorizer()` and `TfidfTransformer()`. It takes care of:\n",
    "   - Tokenizes text and gets rid of stop words and punctuation\n",
    "   - Builds a token count vector \n",
    "   - Converts raw token count into TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# pipeline! See textbook. \n",
    "model = make_pipeline(TfidfVectorizer(), MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "model.fit(train.data, train.target)\n",
    "\n",
    "# predict labels on test data\n",
    "labels = model.predict(test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(labels)\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.target[1]\n",
    "test.target_names[0]\n",
    "test.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seems to match up pretty well\n",
    "test.target[:10]\n",
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat1 = confusion_matrix(test.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test.target, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(mat1.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=train.target_names, yticklabels=train.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you run into this issue with top and bottom rows being cut off,\n",
    "# it's because of a matplotlib version issue (Thanks StackOverflow!).\n",
    "# You'll have to explicitly widen the y-axis, as below.\n",
    "\n",
    "ax = sns.heatmap(mat1, annot=True) #notation: \"annot\" not \"annote\"\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = ['sending a payload to the ISS', 'I met Santa Claus once']\n",
    "preds = model.predict(tests)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.target_names[1])\n",
    "print(train.target_names[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
